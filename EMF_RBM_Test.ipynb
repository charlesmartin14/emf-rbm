{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMF RBM Class Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    " - redo init_weights, in partial_fit...or keep ?\n",
    " - write emf_rbm.py  \n",
    " - fix divide_by_zero in means_hiddens\n",
    " - add test between instead of almost\n",
    " \n",
    " - check mnist\n",
    " - do epochs diverge\n",
    "  - how does julia code,BernoulliRBM behave\n",
    " \n",
    " - second derivative!!!\n",
    " \n",
    " \n",
    " ## TODO; check log likelihood as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T12:16:57.233968",
     "start_time": "2016-10-06T12:16:56.023501"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-01T21:30:48.445229",
     "start_time": "2016-10-01T21:30:48.440001"
    }
   },
   "source": [
    "### use julia data set\n",
    "\n",
    "I don't know how to reproduce their normalization yet\n",
    "\n",
    "TODO: write after it is debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:16:11.990928",
     "start_time": "2016-10-06T17:16:11.973456"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting emf_rbm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile emf_rbm.py\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from sklearn.utils.fixes import expit  # logistic function  \n",
    "from sklearn.utils.extmath import safe_sparse_dot, log_logistic, softmax\n",
    "\n",
    "class EMF_RBM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extended Mean Field Restricted Boltzmann Machine (RBM).\n",
    "    A Restricted Boltzmann Machine with binary visible units and\n",
    "    binary hidden units. Parameters are estimated using the Extended Mean\n",
    "    Field model, based on the TAP equations\n",
    "    Read more in the :ref:`User Guide <rbm>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of binary hidden units.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for weight updates. It is *highly* recommended\n",
    "        to tune this hyper-parameter. Reasonable values are in the\n",
    "        10**[0., -3.] range.\n",
    "    batch_size : int, optional\n",
    "        Number of examples per minibatch.\n",
    "    momentum : float, optional\n",
    "        gradient momentum parameter\n",
    "    decay : float, optional\n",
    "        decay for weight update regularizer\n",
    "    weight_decay: string, optional []'L1', 'L2', None]\n",
    "        weight update regularizer\n",
    "\n",
    "    neq_steps: int, optional\n",
    "        Number of equilibration steps\n",
    "    n_iter : int, optional\n",
    "        Number of iterations/sweeps over the training dataset to perform\n",
    "        during training.\n",
    "    sigma: float, optional\n",
    "        variance of initial W weight matrix\n",
    "    thresh: float, optional\n",
    "        threshold for values in W weight matrix, vectors\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode.\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    h_bias : array-like, shape (n_components,)\n",
    "        Biases of the hidden units.\n",
    "    v_bias : array-like, shape (n_features,)\n",
    "        Biases of the visible units.\n",
    "    W : array-like, shape (n_components, n_features)\n",
    "        Weight matrix, where n_features in the number of\n",
    "        visible units and n_components is the number of hidden units.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    >>> model = EMF_RBM(n_components=2)\n",
    "    >>> model.fit(X)\n",
    "    EmfRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
    "           random_state=None, verbose=0)\n",
    "    References\n",
    "    ----------\n",
    "    [1] Marylou GabrieÂ´, Eric W. Tramel1 and Florent Krzakala1, \n",
    "        Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy\n",
    "        https://arxiv.org/pdf/1506.02914\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=256, learning_rate=0.005, batch_size=100, sigma=0.001, neq_steps = 3,\n",
    "                 n_iter=20, verbose=0, random_state=None, momentum = 0.5, decay = 0.01, weight_decay='L1', thresh=1e-8):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.neq_steps = neq_steps\n",
    "\n",
    "        # learning rate / mini_batch\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # threshold for floats\n",
    "        self.thresh = thresh\n",
    "\n",
    "        # store in case we want to reset\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "        # self.random_state_ = random_state\n",
    "        # always start with new random state\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "        # h bias\n",
    "        self.h_bias = np.zeros(self.n_components, )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "        # moved to fit\n",
    "        \n",
    "        self.W = None\n",
    "        self.dW_prev = None\n",
    "        self.W2 = None\n",
    "        self.v_bias = None\n",
    "        \n",
    "\n",
    "    def init_weights(self, X):\n",
    "        \"\"\" If the user specifies the training dataset, it can be useful to                                                                                   \n",
    "        initialize the visibile biases according to the empirical expected                                                                                \n",
    "        feature values of the training data.                                                                                                              \n",
    "\n",
    "        TODO: Generalize this biasing. Currently, the biasing is only written for                                                                         \n",
    "               the case of binary RBMs.\n",
    "        \"\"\"\n",
    "        # \n",
    "        eps = self.thresh\n",
    "\n",
    "        # Mean across  samples \n",
    "        if issparse(X):\n",
    "            probVis = csr_matrix.mean(X, axis=0)\n",
    "        else:\n",
    "            probVis = np.mean(X,axis=0)            \n",
    "\n",
    "        # safe for CSR / sparse mats ?\n",
    "        # do we need it if we use softmax ?\n",
    "        probVis[probVis < eps] = eps            # Some regularization (avoid Inf/NaN)  \n",
    "        #probVis[probVis < (1.0-eps)] = (1.0-eps)   \n",
    "        self.v_bias = np.log(probVis / (1.0-probVis)) # Biasing as the log-proportion\n",
    "        \n",
    "        # (does not work)\n",
    "        # self.v_bias = softmax(probVis)\n",
    "        \n",
    "        \n",
    "        # initialize arrays to 0\n",
    "        self.W = np.asarray(\n",
    "            self.random_state.normal(\n",
    "                0,\n",
    "                self.sigma,\n",
    "                (self.n_components, X.shape[1])\n",
    "            ),\n",
    "            order='fortran')\n",
    "\n",
    "        self.dW_prev = np.zeros_like(self.W)\n",
    "        self.W2 = self.W*self.W\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def sample_layer(self, layer):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v) or P(v|h)\"\"\"\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        sample = (self.random_state.random_sample(size=layer.shape) < layer) \n",
    "        return sample\n",
    "\n",
    "    def _sample_hiddens(self, v):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer.\n",
    "        \"\"\"\n",
    "        return self.sample_layer(self._mean_hiddens(v))\n",
    "\n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Computes the conditional probabilities P(h=1|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        \"\"\"\n",
    "        p = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def _sample_visibles(self, h):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        return sample_layer(self._mean_visible(h))\n",
    "\n",
    "    def _mean_visibles(self, h):\n",
    "        \"\"\"Computes the conditional probabilities P(v=1|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        Returns\n",
    "        -------\n",
    "         v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.     \n",
    "        \"\"\"\n",
    "        #p = np.dot(h, self.W) + self.v_bias\n",
    "        p = safe_sparse_dot(h, W) + self.v_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def sigma_means(self, x, b, W):\n",
    "        \"\"\"helper class for computing Wx+b \"\"\"\n",
    "        a = safe_sparse_dot(x, W.T) + b\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def init_batch(self, vis):\n",
    "        \"\"\"initialize the batch for EMF only\"\"\"\n",
    "        v_pos = vis\n",
    "        v_init = v_pos\n",
    "\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        h_init = h_pos\n",
    "\n",
    "        return v_pos, h_pos, v_init, h_init\n",
    "\n",
    "    def equilibrate(self, v0, h0, iters=3):\n",
    "        \"\"\"Run iters steps of the TAP fixed point equations\"\"\"\n",
    "        mv = v0\n",
    "        mh = h0\n",
    "     \n",
    "        for i in range(iters):\n",
    "            mv = 0.5 *self.mv_update(mv, mh) + 0.5*mv\n",
    "            mh = 0.5 *self.mh_update(mv, mh) + 0.5*mh\n",
    "        return mv, mh\n",
    "\n",
    "    def mv_update(self, v, h):  \n",
    "        \"\"\"update TAP visbile magnetizations, to second order\"\"\"\n",
    "        \n",
    "        # a = np.dot(h, self.W) + self.v_bias\n",
    "        a = safe_sparse_dot(h, self.W) + self.v_bias\n",
    "\n",
    "        h_fluc = h-(h*h)\n",
    "        a += h_fluc.dot(self.W2)*(0.5-v)\n",
    "        #a += safe_sparse_dot(h_fluc,self.W2)*(0.5-v)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def mh_update(self, v, h):\n",
    "        \"\"\"update TAP hidden magnetizations, to second order\"\"\"\n",
    "        a = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "\n",
    "        v_fluc = (v-(v*v))\n",
    "        a += v_fluc.dot((self.W2).T)*(0.5-h)\n",
    "        #a += safe_sparse_dot(v_fluc,self.W2.T)*(0.5-h)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "\n",
    "    def weight_gradient(self, v_pos, h_pos ,v_neg, h_neg):\n",
    "        \"\"\"compute weight gradient of the TAP Free Energy, to second order\"\"\"\n",
    "        # naive  / mean field\n",
    "        dW = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T - np.dot(h_neg.T, v_neg)\n",
    "\n",
    "        # tap2 correction\n",
    "        h_fluc = (h_neg - (h_neg*h_neg)).T\n",
    "        v_fluc = (v_neg - (v_neg*v_neg))\n",
    "        #  dW_tap2 = h_fluc.dot(v_fluc)*self.W\n",
    "        dW_tap2 = safe_sparse_dot(h_fluc,v_fluc)*self.W\n",
    "\n",
    "        dW -= dW_tap2\n",
    "        return dW\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    def _free_energy_TAP(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1))\n",
    "        \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        \n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))     \n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                   \n",
    "        h_fluc = (mh - (mh*mh))\n",
    "        v_fluc = (mv - (mv*mv))\n",
    "        dW_tap2 = h_fluc.dot(self.W2).dot(v_fluc.T)\n",
    "        Onsager = -0.5*(dW_tap2).sum(axis=1)\n",
    "\n",
    "        fe_tap = U_naive + Onsager - Entropy\n",
    "\n",
    "        return fe_tap - fe\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "\n",
    "    \n",
    "    def score_samples_TAP(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X using second order TAP\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')      \n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        fe = self._free_energy_TAP(v)\n",
    "        fe_ = self._free_energy_TAP(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    def _corrupt_data(self, v):\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \"\"\"Randomly corrupt one feature in each sample in v.\"\"\"\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               self.random_state.randint(0, v.shape[1], v.shape[0]))\n",
    "        if issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "        return v, v_\n",
    "    \n",
    "    \n",
    "    def score_samples_entropy(self, X):\n",
    "        \"\"\"Compute the entropy of X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : array-like, shape (n_samples,)\n",
    "            Value of the entropy.\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the entropy on X,\n",
    "        then on a randomly corrupted version of X, and returns the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        s = self._entropy(v)\n",
    "        s_ = self._entropy(v_)\n",
    "        return v.shape[1] * (s_ - s)\n",
    "\n",
    "    \n",
    "    #TODO: fix later\n",
    "    def _denoise(m, eps=1e-8):\n",
    "        \"\"\"denoise magnetization\"\"\"\n",
    "      #  m[m < eps] = eps\n",
    "        return m\n",
    "\n",
    "\n",
    "    def _free_energy_TAP(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1))\n",
    "        \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        \n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))     \n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                   \n",
    "        h_fluc = (mh - (mh*mh))\n",
    "        v_fluc = (mv - (mv*mv))\n",
    "        dW_tap2 = h_fluc.dot(self.W2).dot(v_fluc.T)\n",
    "        Onsager = -0.5*(dW_tap2).sum(axis=1)\n",
    "\n",
    "        fe_tap = U_naive + Onsager - Entropy\n",
    "\n",
    "        return fe_tap - fe\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "    \n",
    "    \n",
    "    def _entropy(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : array-like, shape (n_samples,)\n",
    "            The value of the entropy.\n",
    "        \"\"\"\n",
    "         \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        \n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                         \n",
    "        return Entropy\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        ## remove this \n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        if not hasattr(self, 'random_state_'):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        if not hasattr(self, 'W'):\n",
    "            self.W = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'h_bias'):\n",
    "            self.h_bias = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'v_bias'):\n",
    "            self.v_bias = np.zeros(X.shape[1], )\n",
    "\n",
    "        # not used ?\n",
    "        #if not hasattr(self, 'h_samples_'):\n",
    "        #    self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X)\n",
    "\n",
    "    def _fit(self, v_pos):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Extended Mean Field theory (second order TAP equations).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_pos : array-like, shape (n_samples, n_features)\n",
    "            The data to use for training.\n",
    "        \"\"\"\n",
    "        X_batch = v_pos\n",
    "        \n",
    "        lr = float(self.learning_rate) / X_batch.shape[0]\n",
    "        decay = self.decay\n",
    "\n",
    "        v_pos, h_pos, v_init, h_init = self.init_batch(X_batch)\n",
    "      \n",
    "        a = safe_sparse_dot(h_init, self.W) + self.v_bias\n",
    "        a = expit(a, out=a)\n",
    "\n",
    "        # get_negative_samples\n",
    "        v_neg, h_neg = self.equilibrate(v_init, h_init, iters=self.neq_steps) \n",
    "        \n",
    "        # basic gradient\n",
    "        dW = self.weight_gradient(v_pos, h_pos ,v_neg, h_neg) \n",
    "\n",
    "        # regularization based on weight decay\n",
    "        #  similar to momentum >\n",
    "        if self.weight_decay == \"L1\":\n",
    "            dW -= decay * np.sign(self.W)\n",
    "        elif self.weight_decay == \"L2\":\n",
    "            dW -= decay * self.W\n",
    "\n",
    "        # can we use BLAS here ?\n",
    "        # momentum\n",
    "        # note:  what do we do if lr changes per step ? not ready yet\n",
    "        dW += self.momentum * self.dW_prev  \n",
    "        \n",
    "        # update\n",
    "        self.W += lr * dW \n",
    "\n",
    "        # storage for next iteration\n",
    "\n",
    "        # is this is a memory killer \n",
    "        self.dW_prev =  dW  \n",
    "        \n",
    "        # is this wasteful...can we avoid storing 2X the W mat ?\n",
    "        self.W2 = self.W*self.W\n",
    "\n",
    "        # update bias terms\n",
    "        self.h_bias += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.v_bias += lr * (np.asarray(v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0))\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        verbose = self.verbose\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \n",
    "        self.init_weights(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        \n",
    "\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        \n",
    "        begin = time.time()\n",
    "        for iteration in xrange(1, self.n_iter + 1):\n",
    "            #print \"iter \", iteration\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice])\n",
    "\n",
    "            #print \"batches done\"\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            The data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape (n_samples, n_components)\n",
    "            Latent representations of the data.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        return self._mean_hiddens(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:52:34.845155",
     "start_time": "2016-10-03T20:52:34.841587"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:35:48.241019",
     "start_time": "2016-10-03T20:35:48.236692"
    }
   },
   "source": [
    "# EMF Tests\n",
    "\n",
    "Designed to match results of julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:32:59.404629",
     "start_time": "2016-10-03T20:32:59.402070"
    }
   },
   "source": [
    "## Test EMF class init\n",
    "\n",
    "Xdigits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:16:42.711544",
     "start_time": "2016-10-06T17:16:42.585981"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.utils.validation import assert_all_finite\n",
    "from scipy.sparse import csc_matrix, csr_matrix, lil_matrix\n",
    "from sklearn.utils.testing import (assert_almost_equal, assert_array_equal,\n",
    "                                   assert_true)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "np.seterr(all='warn')\n",
    "\n",
    "Xdigits = load_digits().data\n",
    "Xdigits -= Xdigits.min()\n",
    "Xdigits /= Xdigits.max()\n",
    "\n",
    "b = Binarizer(threshold=0.001, copy=True)\n",
    "Xdigits = b.fit_transform(Xdigits)\n",
    "print Xdigits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset for julia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:08:43.534724",
     "start_time": "2016-10-03T21:08:43.529474"
    }
   },
   "source": [
    "X = Xdigits.copy()\n",
    "hf =  h5py.File('xdigits.h5',\"w\")\n",
    "hf.create_dataset(\"X\", data=X) \n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:08:50.521176",
     "start_time": "2016-10-03T21:08:50.513828"
    }
   },
   "source": [
    "hf =  h5py.File('xdigits.h5',\"r\")\n",
    "print(\"keys\",hf.keys())\n",
    "X = np.array(hf.get('X'))\n",
    "hf.close()\n",
    "print \"norm of X \",np.linalg.norm(X,ord=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### julia  xdigits_ex.jl   results checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:37:07.818848",
     "start_time": "2016-10-03T21:37:07.813802"
    }
   },
   "source": [
    "### test init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:16:44.654243",
     "start_time": "2016-10-06T17:16:44.632735"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_init():\n",
    "    X = Xdigits.copy()\n",
    "    assert_almost_equal(np.linalg.norm(X,ord=2), 211.4983270228649  , decimal=12)\n",
    "\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=50 , decay=0.01, learning_rate=0.005, n_iter=0, sigma=0.001, neq_steps=3, verbose=True)\n",
    "    rbm.fit(X)\n",
    "    assert_true(np.linalg.norm(rbm.h_bias, ord=2)==0.0)\n",
    "    assert_true(np.linalg.norm(rbm.lr)==0.005)\n",
    "    assert_true(np.linalg.norm(rbm.momentum)==0.5)\n",
    "    assert_true(np.linalg.norm(rbm.decay)==0.01)\n",
    "    assert_true(np.linalg.norm(rbm.n_iter)==0)\n",
    "    assert_true(np.linalg.norm(rbm.neq_steps)==3)\n",
    "    assert_true(np.linalg.norm(rbm.sigma)==0.001)\n",
    "    assert_true(np.linalg.norm(rbm.verbose)==True)\n",
    "    assert_true(np.linalg.norm(rbm.n_components)==64)\n",
    "    assert_true(np.linalg.norm(rbm.thresh)==1e-8)\n",
    "    assert_true(np.linalg.norm(rbm.batch_size)==50)\n",
    "\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.97455, decimal=5)\n",
    "\n",
    "    #assert_true(np.linalg.norm(rbm.weight_decay)=='L1')\n",
    "    assert_array_equal(X, Xdigits)\n",
    "    \n",
    "test_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one epoch\n",
    "\n",
    "compare:\n",
    "\n",
    "5 julia runs  \n",
    "batch norm of W, hb, vb  0.015177951725370209 6.125160958113443e-5 38.974531344645186  \n",
    "batch norm of W, hb, vb  0.016005072745766846 6.132506125735679e-5 38.974534343561935  \n",
    "batch norm of W, hb, vb  0.015518275427920199 6.143705375221393e-5 38.97453267232916\n",
    "batch norm of W, hb, vb  0.016618832753491925 6.14604623830071e-5 38.97453303623846\n",
    "batch norm of W, hb, vb  0.015643733669880935 6.131198883353152e-5 38.97453109464897\n",
    "\n",
    "10 BernoulliRBM runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:42:30.688055",
     "start_time": "2016-10-06T17:42:30.574002"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_one_epoch():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=1, \n",
    "                  sigma=0.001, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974531, decimal=4)\n",
    "    # really between 0.015 and 0.0165: hard to test properly with a single statement\n",
    "\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.0165, decimal=2)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.000061, decimal=2)\n",
    "    \n",
    "    # non tap FE totally wrong\n",
    "    # FE ~ -2x.x\n",
    "\n",
    "    scored_free_energy = np.average(rbm.score_samples(X))\n",
    "    \n",
    "    avg_free_energy_tap = np.average(rbm._free_energy_TAP(X))\n",
    "    avg_entropy = np.average(np.average(rbm._entropy(X)))\n",
    "    \n",
    "    #assert_almost_equal(scored_free_energy, -24, decimal=0)  \n",
    "    #assert_almost_equal(avg_free_energy_tap, -25, decimal=0)  \n",
    "    assert_almost_equal(avg_entropy, 68.8, decimal=0)\n",
    "    return rbm\n",
    "\n",
    "for i in range(1):\n",
    "    test_one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:09:44.721707",
     "start_time": "2016-10-06T17:09:44.719076"
    }
   },
   "source": [
    "###  Exact Result, $\\sigma\\sim0$\n",
    "\n",
    "julia result:\n",
    "\n",
    "entropy: 68.80871848695904\n",
    "TAP free_energy: -23.272298000408423\n",
    "\n",
    "updated h_bias 6.073000902126526e-5  \n",
    "updated v_bias 38.97454137433605\n",
    "\n",
    "### <font color='red'>we are off by 0.1 ?  why </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:55:59.965424",
     "start_time": "2016-10-06T17:55:59.093419"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23.8945530271 -24.1352418163 68.5059859099 38.9745455624 6.08993824893e-05\n",
      "-23.4472235728 -24.1352420184 68.505978328 38.9745455436 6.08994477951e-05\n",
      "-24.6073456474 -24.1352419756 68.5059835881 38.9745455543 6.08981783802e-05\n",
      "-25.3625787431 -24.1352418721 68.5059866645 38.9745455633 6.08987185997e-05\n",
      "-23.8287090865 -24.1352412005 68.5059800141 38.9745455481 6.08999623779e-05\n",
      "-24.1339629159 -24.1352412676 68.5059818404 38.9745455521 6.08994605687e-05\n",
      "-25.414405966 -24.1352424659 68.5059761111 38.9745455385 6.08979186068e-05\n",
      "-24.2108426679 -24.1352413658 68.5059849335 38.9745455591 6.08998762331e-05\n",
      "-25.0368441183 -24.135241446 68.5059796102 38.9745455455 6.08994626719e-05\n",
      "-25.9144173308 -24.1352410762 68.505980824 38.9745455499 6.08991184307e-05\n"
     ]
    }
   ],
   "source": [
    "def test_one_epoch():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=1, \n",
    "                  sigma=1e-16, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "\n",
    "    scored_free_energy = np.average(rbm.score_samples(X))\n",
    "    \n",
    "    avg_free_energy_tap = np.average(rbm._free_energy_TAP(X))\n",
    "    avg_entropy = np.average(np.average(rbm._entropy(X)))\n",
    "\n",
    "    print scored_free_energy, avg_free_energy_tap, avg_entropy, np.linalg.norm(rbm.v_bias, ord=2), np.linalg.norm(rbm.h_bias, ord=2)\n",
    "    return rbm\n",
    "\n",
    "for i in range(10):\n",
    "    test_one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T22:07:51.818339",
     "start_time": "2016-10-03T22:07:51.814845"
    }
   },
   "source": [
    "### test partial fit  (1 iter, 1 batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:09:54.937747",
     "start_time": "2016-10-06T15:09:54.400217"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_partial_fit():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=0.000000001, neq_steps=3, verbose=True)\n",
    "    rbm.init_weights(X);\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.9745518)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W2,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2), 0.000000001)\n",
    "\n",
    "    X_batch = Xdigits.copy()[0:100]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.3103298921)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.007629, decimal=4)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974521, decimal=4)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0, decimal=3)    \n",
    "    \n",
    "    #there are large variations in dw_prev\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),152.6, decimal=1)\n",
    "\n",
    "# test stochastically (sometimes will fail due to roundoff error in dw_prev)\n",
    "for i in range(100):\n",
    "    test_partial_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T16:04:44.744505",
     "start_time": "2016-10-04T16:04:44.742373"
    }
   },
   "source": [
    "### Test 2 iterations of the partial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:09:56.380427",
     "start_time": "2016-10-06T15:09:55.808980"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_partial_fit_2iters():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=0.00000001, neq_steps=3, verbose=True)\n",
    "    rbm.init_weights(X);\n",
    "    X_batch = Xdigits.copy()[0:100]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.3103298921)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    \n",
    "    X_batch = Xdigits.copy()[100:200]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 48.96867960939811)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974504602)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.000001, decimal=6)\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.0154, decimal=3)\n",
    "    # not correct ?\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),177.75, decimal=1)\n",
    "   \n",
    "\n",
    "# test stochastically\n",
    "for i in range(100):\n",
    "    test_partial_fit_2iters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:09:59.555197",
     "start_time": "2016-10-06T15:09:57.145138"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch [ 0.920512066852 0.961619286439 ] 0.0411072195871 4.27479150708 %\n",
      "20 epochs [ 0.918229195788 0.965514647552 ] 0.0472854517646 4.89743494668 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "w_norms = []\n",
    "for i in range(100):\n",
    "    rbm1 = BernoulliRBM(n_components=64, batch_size=100,\n",
    "                  learning_rate=0.005, n_iter=1,  verbose=False)\n",
    "    X = Xdigits.copy()\n",
    "    rbm1.fit(X);\n",
    "    w_norms.append(np.linalg.norm(rbm1.components_,ord=2))\n",
    "    \n",
    "diff= max(w_norms)-min(w_norms)\n",
    "print \"1 epoch [\",min(w_norms),max(w_norms),\"]\", diff, 100.0*diff/max(w_norms),\"%\"\n",
    "\n",
    "w_norms = []\n",
    "for i in range(100):\n",
    "    rbm1 = BernoulliRBM(n_components=64, batch_size=100,\n",
    "                  learning_rate=0.005, n_iter=1,  verbose=False)\n",
    "    X = Xdigits.copy()\n",
    "    rbm1.fit(X);\n",
    "    w_norms.append(np.linalg.norm(rbm1.components_,ord=2))\n",
    "\n",
    "diff= max(w_norms)-min(w_norms)\n",
    "print \"20 epochs [\",min(w_norms),max(w_norms),\"]\", diff, 100.0*diff/max(w_norms),\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So |W| can vary uptp %5, even after 20 epochs  \n",
    "and the variation across runs >> variation across epochs\n",
    "\n",
    "which make it difficult to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fit 20 epochs on xdigits\n",
    "\n",
    "### <font color='red'>energies are not deterministic .  need to remove noise and check </font>\n",
    "\n",
    "## TODO; check log likelihood as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:19:44.045186",
     "start_time": "2016-10-06T15:19:43.420883"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_fit_xdigits():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=20, \n",
    "                  sigma=0.001, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "    \n",
    "    \n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.02, decimal=1)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.9747, decimal=3)\n",
    "    # why is h so different ?\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0012, decimal=2)\n",
    "    return rbm\n",
    "    \n",
    "rbm = test_fit_xdigits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the varation in the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:40.317590",
     "start_time": "2016-10-06T15:19:44.046713"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_s, vb_s, hb_s = [], [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    rbm = test_fit_xdigits();\n",
    "    w_s.append(np.linalg.norm(rbm.W, ord=2))\n",
    "    vb_s.append(np.linalg.norm(rbm.v_bias, ord=2))\n",
    "    hb_s.append(np.linalg.norm(rbm.h_bias, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:40.638613",
     "start_time": "2016-10-06T15:27:40.319354"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean |W|  0.0203914707183\n",
      "std |W|  0.000350792430527 1.72028999464  %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112daec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(w_s,50);\n",
    "plt.title(\"distribution of |W|\")\n",
    "print \"mean |W| \", np.mean(np.array(w_s))\n",
    "print \"std |W| \", np.std(np.array(w_s)), 100.0*np.std(np.array(w_s))/np.mean(np.array(w_s)), \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:40.974751",
     "start_time": "2016-10-06T15:27:40.640289"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11cb96c50>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEXCAYAAACpuuMDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGsBJREFUeJzt3XuYJXV95/H3ZxgYBQQBnWkRnPHCbXVRvKAmGpvgDVmF\nxASVRxdwjbv7GHXjuhFclYlxDWZ3H2/J8+RJdHXMigJGAhqVEZnOBhGBAIqA45WbOJ3l4gVwvc13\n/zjVzKmme6b79OlzTne/X89znq5TVb+qb1XXOd9Tv9+vqlJVSJI0ZdWwA5AkjRYTgySpxcQgSWox\nMUiSWkwMkqQWE4MkqcXEIElqMTHoAZJ8JMk7m+FnJbmxj8v+XJJXNcOnJPmnPi775CRf6Nfy5rHe\n30jyrSQ/SfKSGaafmeQdC1zHliSvnmXawc26s8B1LDhOLQ+rhx2ARltVXQocsav5kpwJPLaq/u0u\nlvei6aN6iSvJeuD7wOqq2t4s+2zg7F6Wt0DvBD5QVX8xhHVTVbcC+wxj3VqePGPQwCz0F+30xdFJ\nKv1cZq/WAzcMOwipX0wMIslRSf45yY+TfBJ4UNe05yS5tev9W5Lc1lRd3JjkmCQvAN4KvCzJT5Nc\n08y7Jcm7klya5F7g0TNUiaxK8sEkP0pyQ5Lf7lrX96e9PzPJx5q3/9j8/VETy9OnV001VTxXJLk7\nyVeTPLNr2pYk72xi+0mSLyTZfyf76A+SfDvJHUn+PslYM/47wKOBzzbL2X0X+/qPk5w3bdz7k7xv\nZ+Uaj2u248dJzk/y0Kb8+iTbk6xq3p/a7MufJPlOktd2reuAJJ9p9smdSf5xtpVp5TIxrHDNF9n5\nwCZgf+A84KXTZqtm3kOB1wFPqap9gBcAN1XVRcC7gXOq6iFVdVRX2VcCrwEeAtwyQwhPB74NHABs\nBD499YW3C7/V/N2nqvapqq9Oi3U/4LPA+5plvxf4h2b8lFcApwAPB9YAb55pRU1yejfwe8Ajmu04\nB6CqHgfcChzfxPHLXcT9SeC4JHs1y14F/D7w8Tls86uAU4Ex4NfAB7umdVfJTQIvav5HpwHvTfKk\nZtp/buI9AFhLJ6FLLSYGPYNOPf0HqurXVfV3wJWzzPtrYA/gCUlWV9UtVfX9XSz/o1X1zaraXlW/\nmmH6ZNe6zwW2AsfPI/7ZqpKOB75VVWc36/4k8E3gxV3zfKSqvltVPwfOBZ4004KAk4EPV9XXmi/+\nM4BnJnnUHOJoqapbgKuB32lGHQvcW1Wz7fNuf1tVN1bVz4C3AyfNVD1XVZ+vqpua4X8CNgPPbib/\nkk5ye3Szz788l7i1spgYdCDwg2njbp5pxqr6LvCf6Pyyn0xy9lSVyk7cuovpM637wF2UmYsDeeB2\n3Aw8suv9tq7h+4C957KsqroXuHPasubjE3TOVmj+zrXBvHtf3gzsDjxs+kxJjkvylaaq6G7guK75\n/jvwXWBzU830ll42QMubiUE/5IFfcI+aaUaAqvpkVT2bToMrwHumJs1WZBfrn2ndtzfD9wJ7dk3r\nTkK7Wu7twIYZlj09Ec3F7ezYXppqoAOA23pYFnSq68aTPJLOmcNcE8PBXcPrgV8Ad3TPkGQP4FPA\nnwMPr6r9gM/TnNFU1T1V9eaqeizwEuBNSY7pcTu0TJkY9BXgV0len2R1kt8Fjp5pxiSHNo3Ne9D5\nUvoZsL2ZPAls6KHn0bqudf8+cDjwuWbatcDLm2lPpVPHP+X/Nut+7CzL/RxwSJKXJ9ktycvodLv9\nzDzjg84v/NOSHJlkDZ32hsubbqLzVlV30Gk8/wjwvaraOseir0xyeJI9gT8BzqsdD1SZ2u97NK87\nqmp7kuOA508tIMnxSab22U+BX7HjfygBPSSG5svhmiRXN39/nOQNSfZLsjnJ1iQXJdl3MQJWfzV1\n5r9Lp5HyTjoNoX83y+xrgLPofCnfTqfR9oxm2nl0vpzuTHLV1OJnWuW095cDh9D55funwEur6u5m\n2tuBxwF3AWfS1UDb1LP/N+DLSe5K0kpmVXUX8G/oNCjf0fw9vmvZc75+oqq+1MTyaTpnHI8GXr6T\nbZqLs+m0L8yl0XlqHX9Lp5PA7XS+/N84PYaqugd4A3BekruaOC/omu8Q4OIkPwW+DPxlVdkzSS1Z\nyBPcmh4Vt9HpWfKHwJ1V9edNveV+VXV6f8KUlq50Lv6rqnrnsGPZmaUSpxbfQquSngt8tzmlPoHO\nrxmavycucNmSpCFY6C0xXsaOhrN1VTUJUFXbkqxd4LKl5WLLXGZqqne6T+Gnru4+bkDdSucUp5a/\nnquSmgujbgeOqKo7ktxVVft3Tb+zqg7oU5ySpAFZyBnDccA/Nz0soNOvfV1VTTZ92/9lpkJJem/U\nkKQVrKoGcm+whbQxvIJON74pF9K5XB86txm4YHqBKVXlq4ozzzxz6DGMyst94b5wX+z8NUg9JYam\nH/Vz6XTfm/Ie4HlJttLphnfWwsOTJA1aT1VJVXUfnT7s3ePuopMsJElLmFc+D9H4+PiwQxgZ7osd\n3Bc7uC+GY0EXuPW0wqQGvU5JWuqSUEug8VmStAyZGCRJLSYGSVKLiUGS1GJikCS1mBgkSS0mBi0r\nY2MbSDLja2xsw7DDk5YEr2PQstJ5suhsx1cGfs8ZqV+8jkGSNDQmBklSi4lBktRiYpAktZgYJEkt\nJgZJUouJQZLUYmKQJLWYGCRJLSYGSVKLiUGS1GJikCS1mBgkSS09JYYk+yY5L8mNSa5P8vQk+yXZ\nnGRrkouS7NvvYCVJi6/XM4b3A5+rqiOAJwLfBE4HLq6qw4BLgDP6E6IkaZDm/TyGJPsA11TVY6eN\n/ybwnKqaTDIGTFTV4TOU93kMWjQ+j0HL1ag/j+HRwB1JPpLk6iR/nWRPYF1VTQJU1TZgbT8DlSQN\nxuoeyzwZeF1VXZXkvXSqkab/FJv1p9nGjRvvHx4fH2d8fLyHMLTcjY1tYHLy5hmnrVu3nm3bbprn\nEtc0ZxT9Wp60eCYmJpiYmBjKunupSloHfKWqHtO8fxadxPBYYLyrKmlL0wYxvbxVSZqTXqqFdlXG\naiYtVSNdldRUF92a5NBm1LHA9cCFwKnNuFOAC/oRoCRpsOZ9xgCQ5InAh4Ddge8BpwG7AecCBwM3\nAydV1Y9mKOsZg+bEMwZph0GeMfSUGBa0QhOD5sjEIO0w0lVJkqTlzcQgSWoxMUiSWkwMkqQWE4Mk\nqcXEIElqMTFIklpMDJKkFhODJKnFxCBJajExSJJaTAySpBYTgySpxcQgSWoxMUiSWkwMkqQWE4Mk\nqcXEIElqMTFIklpMDJKkFhODJKnFxCBJajExSJJaVvdSKMlNwI+B7cAvq+roJPsB5wDrgZuAk6rq\nx32KU5I0IL2eMWwHxqvqqKo6uhl3OnBxVR0GXAKc0Y8AJUmD1WtiyAxlTwA2NcObgBN7DUqSNDy9\nJoYCvpjkyiSvacatq6pJgKraBqztR4CSpMHqqY0B+M2q+mGShwObk2ylkyy6TX9/v40bN94/PD4+\nzvj4eI9haOVaQ5JFX96qVXuyfft9M5ZYt24927bd1McYpB0mJiaYmJgYyrpTNev399wWkJwJ3AO8\nhk67w2SSMWBLVR0xw/y10HVqZeh8Uc92rMw2rZcyvS/PY1mDkoSq6uevoVnNuyopyZ5J9m6G9wKe\nD1wHXAic2sx2CnBBn2KUJA1QL1VJ64Dzk1RT/uNVtTnJVcC5SV4N3Ayc1Mc4JUkDsuCqpHmv0Kok\nzZFVSdIOI12VJEla3kwMkqQWE4PUZ2NjG0gy42tsbMOww5N2yTYGjayl2sawq7g9/tUL2xgkSUNj\nYpAktZgYJEktJgZJUouJQZLUYmKQJLWYGCRJLSYGDYQXfUlLhxe4aSB6uejLC9ykHbzATZI0NCYG\nSVKLiUGS1GJikCS1mBgkSS0mBklSi4lBktRiYpAktZgYpBHgleEaJV75rIHwyuf+l9HKsiSufE6y\nKsnVSS5s3u+XZHOSrUkuSrJv/8KUJA3KQqqS3gjc0PX+dODiqjoMuAQ4YyGBScvTmhmri6RR0lNi\nSHIQ8CLgQ12jTwA2NcObgBMXFpq0HP2cTpXR9Jc0Ono9Y3gv8F9oH9HrqmoSoKq2AWsXGJskaQhW\nz7dAkuOByaq6Nsn4Tmad9WfQxo0b7x8eHx9nfHxni9GoGRvbwOTkzTNOW7duPdu23TTYgIZmjdVA\nWjQTExNMTEwMZd3z7pWU5N3AK4FfAQ8GHgKcDzwVGK+qySRjwJaqOmKG8vZKWuIG1etmKfRKGlQZ\nPzMa6V5JVfXWqnpUVT0GeDlwSVW9CvgMcGoz2ynABX2LUpI0MP28wO0s4HlJtgLHNu8lSUuMF7hp\n3qxKGnwZPzMa6aokSdLyZmKQJLXMu7uqtHN24ZSWOhOD+mzqyt7pTBbSUmFVkiSpxcQgSWoxMUiS\nWkwMkqQWG581AuzJJI0SE4NGgD2ZpFFiVZIkqcXEIElqMTFIklpMDJKkFhODNPI6vbamv8bGNgw7\nMC1TPo9B8za45yT4PIZdlfGztHL4PAZJ0tCYGCRJLSYGSVKLiUGS1GJikCS1mBgkSS0mBklSy7wT\nQ5I1Sb6a5Jok1yU5sxm/X5LNSbYmuSjJvv0PV5K02OadGKrq58AxVXUU8CTguCRHA6cDF1fVYcAl\nwBl9jVSSNBA9VSVV1X3N4Bo6z3Qo4ARgUzN+E3DigqOTJA1cT4khyaok1wDbgC9W1ZXAuqqaBKiq\nbcDa/oUpSRqUnp7gVlXbgaOS7AOcn+TxPPBmLrPexGXjxo33D4+PjzM+Pt5LGJK0bE1MTDAxMTGU\ndS/4JnpJ3g7cB7wGGK+qySRjwJaqOmKG+b2J3hLnTfRGp4yfpZVjpG+il+RhUz2OkjwYeB5wI3Ah\ncGoz2ynABX2KUZI0QL1UJT0C2JRkFZ3Eck5VfS7J5cC5SV4N3Ayc1Mc4JUkD4vMYNG9WJY1OGT9L\nK8dIVyVJkpY3E4MkqcXEoFmNjW2Y8VnDkpY32xg0q9nbEka73t02Bi1HtjFIkobGxCCtMLNVEY6N\nbRh2aBoRViVpVlYljX6ZXj5LO/u/+tkcXVYlSZKGxsSwws1WrWDvI2nlsipphVt+VzFblbQrViUt\nTVYlSZKGxsQgSWoxMUiSWkwMkqQWE4MkqcXEIElqMTFIklpMDNIy5IWLWggvcFvhvMBtVGLo7wVu\nvf5f/WyOLi9wkyQNjYlBktRiYlghfEynpLmyjWGFWDnPVrCNAWxjWI5Guo0hyUFJLklyfZLrkryh\nGb9fks1Jtia5KMm+/Q9XkrTYeqlK+hXwpqp6PPBM4HVJDgdOBy6uqsOAS4Az+hemJGlQ5p0Yqmpb\nVV3bDN8D3AgcBJwAbGpm2wSc2K8gJUmDs6DG5yQbgCcBlwPrqmoSOskDWLvQ4CRJg7e614JJ9gY+\nBbyxqu5JMr3VatZWrI0bN94/PD4+zvj4eK9hSCvYGnuWLWMTExNMTEwMZd099UpKshr4LPD5qnp/\nM+5GYLyqJpOMAVuq6ogZytoraQjslTTqMYzGtvrZHF0j3Sup8b+AG6aSQuNC4NRm+BTgggXEJUka\nknmfMST5TeD/ANfR+dlRwFuBK4BzgYOBm4GTqupHM5T3jGEIPGMY9RhGY1v9bI6uQZ4xeIHbCmFi\nGPUYRmNb/WyOrqVQlSRJWqZMDMuI9+DXYpnt2Bob2zDs0LQIrEpaRny2Qq9lRiGG0djW+d97yeqn\nQbEqSZI0NCYGSVKLiUGS1GJikCS1mBgkSS0mhhFl90BJw2J31RHVS/dAu6v2WmYUYhiNbbW76uiy\nu6okaWhMDJKkFhODJKnFxCBJaun50Z4aFh/nKGlxmRiWnJ+z894mkrQwViVJklo8Y5DUsJpSHSYG\nSQ2rKdVhVZIkqcXEIElqMTFIklpMDJKklnknhiQfTjKZ5Otd4/ZLsjnJ1iQXJdm3v2FKkgallzOG\njwAvmDbudODiqjoMuAQ4Y6GBSZKGY96JoaouBe6eNvoEYFMzvAk4cYFxSZKGpF9tDGurahKgqrYB\na/u0XEnSgC3WBW47faTTxo0b7x8eHx9nfHx8kcKQNCxjYxuYnLz5AePXrVvPtm03DT6gJWZiYoKJ\niYmhrLunR3smWQ98pqqObN7fCIxX1WSSMWBLVR0xS1kf7TkHO3uUoo+u7HeZUYhh6W6rjwMdjKXw\naM/Qvkb+QuDUZvgU4IIFxCRJGqJeuqueDVwGHJrkliSnAWcBz0uyFTi2ea/G2NgGkjzgNTa2Ydih\nSSPFz8po6KkqaUErXIFVSb2cUluVNMgyoxDD0t3WflYlWf00u6VQlSRJWqa87fZQef97LXUew8uR\niWGovP+9ljqP4eXIqiRJUouJQZLUYmKQJLWYGCRJLSYGSVKLiUHSErBmxiuivSp6cdhdVdISMHu3\n2MlJu8X2m2cMkqQWE4MkqcXEIElqMTFIklpMDPPk/eIlLXc+j2Gehv9shdG4B//yKjMKMaysbe33\nZ2Upf6fMlc9jkCQNzYpODLNVC+22216zXkwjScvdir7AbXLyZmY6Pd2+fVenx5K0fK3oMwZJ0gMt\nqcTQS9XP4HoLzX4vF0lLx6B6Ho5yD8cl1Sup3z0Wht9baGX1RBndMqMQw8ra1lHuldRLz8NBrGfJ\n9kpK8sIk30zyrSRv6eeyJUmD0bfEkGQV8BfAC4DHA69IcvhM877hDWew//4Hz/g6+eTX9iskLSkT\nww5ghEwMO4ARMjHsAFakfp4xHA18u6purqpfAp8ETphpxssuu5K7734Pd9992bTXhzjnnE/0ua7e\nuv+lYWLYAYyQiWEHsMjm85mcWJQIZqvf93uho5/dVR8J3Nr1/jY6yWIWa4GDp427i+3b76G/XUVn\nv4+7XU+lYRj+Z3K2ruqDjGGUDeU6hjVrdmfPPd/G6tXvb43fvv3H3HPPMCKSJE3pW6+kJM8ANlbV\nC5v3pwNVVe+ZNt/yv6mJJC2CQfVK6mdi2A3YChwL/BC4AnhFVd3YlxVIkgaib1VJVfXrJH8IbKbT\nqP1hk4IkLT0Dv8BNkjTa5tRddS4XriX5QJJvJ7k2yZN2VTbJfkk2J9ma5KIk+zbj909ySZKfJvnA\ntHU8OcnXm2W9r7dNXpgR2hdbmmVdk+TqJA9brG2ezYD3xXOTXJXka0muTHJMV5mVdlzsbF+stOPi\nac22Tr1O7Cqz0o6Lne2L+R0XVbXTF53k8R1gPbA7cC1w+LR5jgP+oRl+OnD5rsoC7wH+uBl+C3BW\nM7wn8BvAa4EPTFvPV4GnNcOfA16wq/j7+RqxfbEFOGqQ2z/kffFEYKwZfjxw2wo+Lna2L1bacfEg\nYFUzPAZMdr1facfFzvbFvI6LuZwxzOXCtROAjwFU1VeBfZOs20XZE4BNzfAm4MSm/H1VdRmdzs73\nSzIGPKSqrmxGfWyqzACNxL7oMsybIA56X3ytqrY1w9cDD0qy+wo9LmbcF13rWknHxf+rqu3N+AcD\n22HFfl/MuC+6zPm4mMuMM1249sg5zrOzsuuqahKgOcjXziGO23YRx2IblX0x5aPNaeHb5jh/Pw1t\nXyT5PeDq5gOzoo+Laftiyoo6LpIcneQbwNeA/9B8Oa7I42KWfTFlzsfFYv2y6KWv7XJtBV+sfXFy\nVf1r4NnAs5O8sof1DNqC90WSxwN/Rqd6bSlbrH2x4o6Lqrqiqp4APA14a5I9+hbZ4C3WvpjXcTGX\nxPAD4FFd7w9qxk2f5+AZ5tlZ2W3NKdPUad+/zCGOmdYxSKOyL6iqHzZ/7wXOZqe3H1kUA98XSQ4C\nPg28qqpu2sU6BmlU9sWKPC6mVNVW4B7gCTtZxyCNyr6Y/3ExhwaU3djRCLIHnUaQI6bN8yJ2NKA8\ngx0NKLOWpdOA8pbpDShdyzwF+OC0cZc3GxQ6jUkvnGtjSj9eo7IvmmUd0AzvDpwHvHY57wvgoc18\nJ84Qy4o6LmbbFyv0uNgA7NYMr6dT5bL/Cj0uZtwXvRwXc93AF9K5qvnbwOnNuH/fvXA6t9z+Dp26\nrSfvrGwzfn/g4mbaZuChXdO+D9wB/AS4hR2t8U8BrmuW9f5B/pNHaV/Q6a10VXOwXAe8l+aalOW6\nL4D/CvwUuBq4pvn7sJV4XMy2L1bocfFK4BvNPrgKeHFXmZV2XMy4L3o5LrzATZLUsqSe+SxJWnwm\nBklSi4lBktRiYpAktZgYJC0bSd7Z3FzwmiRfaPr5zzTfHyX5RnOTvY9PXQiW5MgklzXLuCDJ3s34\nk7tuQHdNkl8nOXLaMi9M8vVp405Kcn2S65L87znE/+Ekk9OXM2j2SpK0JCV5DnBqVZ3WNW7vqrqn\nGX498K+q6j9OK3cgcCmdbvC/SHIOnWsJPpbkCuBNVXVpklOBx1TVO6aVfwJwflUd0jXud4CXAkdW\n1ZHNuMcB5wDHVNVPkjysqu7YxTY9i86FaR+bWs4weMYgaSlr/bKdSgqNvXjgjeSm7AbslWQ1nX7+\nU1cVH1pVlzbDF9P5sp/uFXRuagdAkr2APwLeNW2+PwD+sqp+0sR2R1eZNye5ornV9pld8V8K3D1L\nzANjYpC0lD3g3kJJ3pXkFuBk4B3Tp1fV7cD/pHPB6A+AH1XVl5rJ30jykmb4JDq3opjuZcAnut7/\nKfA/gJ9Nm+9Q4LAklzbVUy9o4nsecEhVHQ0cBTy1OVMYGSYGSUtKksuTXA18CHhxU+9/dfOFS1W9\nraoeBXwceP0M5R9K59bV64EDgb2TnNxM/nfA65JcSeeM4xfTyh4N3FtVNzTvnwg8tqoupJOkuhPV\nauBxwG/RSVJ/k2Qf4PnA85ptuBo4DDiEEdK3Zz5L0iBU1TPg/jaGU6rq1bPMejadeyRtnDb+ucD3\nququZjmfpvNArLOrc/O5qV/2hwDHTyv7ctpnC88EnpLke3TuQ7Q2ySVV9dt07lV0eXVufX1Tkm/R\nSQAB/qyq/mbeGz8gnjFIWjaaBt8pJwI3zjDbLcAzkjwoSYBjp+ZL8vDm7yrgbcBfdS07dKqX7m9f\nqKq/qqqDquoxwLOArU1SAPh74Jim7MPoJIXvARcBr27aJkhy4NR6p1ZFb7ff7hsTg6Tl5KymC+q1\ndM4M3giQ5BFJPgudZxYAn6JzA8Kv0fkS/uum/CuSbAVuAH5QVR/tWvZvAbdU123Od6aqLgLuTHI9\n8CXgzVV1d1V9kc7ZzFeabqnnAVPdYs8GLgMOTXJLktNmWfyisruqJKnFMwZJUouJQZLUYmKQJLWY\nGCRJLSYGSVKLiUGS1GJikCS1mBgkSS3/H9AEBj6EhTAGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112fbf810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vb_s,50);\n",
    "plt.title(\"distribution of |v_bias|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.270756",
     "start_time": "2016-10-06T15:27:40.976424"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11cab4690>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEXCAYAAABoPamvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGylJREFUeJzt3XuYJXV95/H3ZxgYBAFHE6a9wXhFNjERL0TirSNRjK7C\nakTNDXSzmzUxZtd1A7ibMJqsK8nu4yU+2U12FcesN4yK94AEOokxeEMIERyJyojKtAJewBFWnO/+\nUdXMme7TMz1T1X36nH6/nuc8XafqV79Lnz717fr9qn6VqkKSpC7WjboCkqTxZzCRJHVmMJEkdWYw\nkSR1ZjCRJHVmMJEkdWYwkSR1ZjDRkiU5P8mr2uXHJ7m2x7w/kuRX2+Uzkvxdj3n/UpK/6iu//Sj3\nZ5N8Mcn3kjxryPZzk/z+wPuvJHnyfpbxpCQ37GX7/0zyn/ev5kPz2dU1D0229aOugMZTVX0cOH5f\n6ZKcCzyoqn5tH/k9ff6qA6lXkmOBrwDrq2pXm/fbgbcfSH4dvQp4Q1W9cZnLWfR3VVUvXu4yJPDM\nRKtAkvSZHc2Br888D9SxwDWjroS0EgwmWlSSE5J8Nsl3k7wTOHRg2x7dK0nOSvK1tkvn2iQ/l+QU\n4BXA85LcmuRzbdrLkvxhko8n+T7wgHbdiwaKX5fkT5J8J8k1g90/87uD2u6it7Zv/6b9+Z22Lj8z\nv9us7X76VJJvJ/lkkpMGtl2W5FVt3b6X5K+S3HMvv6N/k+S6JDcluTDJVLv+n4EHAB9q8zl4ib/2\nE5Jc1dbtHUkOWcI+SXJOkm8l+XKSXxrYMNg1eY8kH0zyzSQ3t8v3HUh7ZpIvtfX9UpIXLLHOksFE\nw7UHv/cBW4F7Au8GnjMvWbVpHwr8FvCoqjoSOAW4vqouAl4NvKuqjqiqEwb2/RXg14EjgK8OqcLP\nANcB9wK2AO9Nco8lVP2J7c8jq+rIqvrkvLpuBD4EvK7N+7XAh9v1c14AnAH8OLABePmwgtqA9mrg\nF4F7t+14F0BVPRi4AXhGW48fLqHuAM8FnkoTiH4aOHMJ+0zRfEb3adP/eZKHDEm3DngzcH/gGGAn\n8Ma2LYcBrwdOaT/DnwWuXGKdJYOJFvVYmnGHN1TVj6rqPcCnF0n7I+AQ4CeTrK+qr1bVV/aR/1uq\n6gtVtauq7hyyfXag7AuAbcAz9qP+i3VzPQP4YlW9vS37ncAXgGcOpDm/qr5UVXcAFwCPWCSvXwLe\nVFVXtcHiHOCkJMcsoR6LeX1VzVbVd4AP7qXsQQX8XlX9sKr+FvgwcPqCRFW3VNX7quqOqvo+8N/Y\nHXyh+RwfnuTQtg69XWChyWcw0WLuA3x93rrtwxJW1ZeAf09zBjGb5O1z3T17segVSK1hZd9nH/ss\nxX1Y2I7twH0H3u8YWN4J3H0pebUH6Jvn5bW/ZpdY9qBvV9XtA++H/q6S3C3JnyW5Psl3aLoE75Ek\nVbUTeB7wYuDGtgvsuANvhtYag4kWcyMLD4rHDEsIUFXvrKon0Aw6A5w3t2mxXfZR/rCyv9Eufx84\nbGDbYODaV77fADYPyXt+8FqKb7C7vSQ5nKbr7GsHkFcXG5PcbeD94O9q0MuBhwCPqap7sPusJABV\n9bGqeirN73Mb8L+Xr8qaNAYTLeYfgDuT/HaS9UmeDZw4LGGSh7YD7ocA/w/4ATB3X8IssPkArtja\nNFD2c4GHAR9pt10JPL/d9miaMYs532rLftAi+X4EeEiS5yc5KMnzaC5x/uB+1g/gHcALk/xUkg00\n4yeXV9W+zrr6FuCVSQ5O8gSarrwLhqS7O81n8732ooItd2WQHJ3kWe3YyQ+B22i6vaQl6TWYJDkq\nybvbq3k+315JszHJxUm2JbkoyVF9lqnl0Y4BPBt4IU3XzXOB9yySfAPwGpoD+TdoBq7Pabe9m+Zg\nd3OSz8xlP6zIee8vp/kv+ibgD4DnVNW3222/BzwYuAU4F3jbQL1/APxX4O+T3JJkjwBYVbcA/5Lm\nv/Sb2p/PGMh7yfdTVNVft3V5L82ZzQOA5++lTfvMcj/Tz7kR+DbN7/4vgN+oquuGpHsdzRndTcAn\n2B2coTkWvIymHTfRnLX0dY+K1oD0+aTFJG8B/qaqzk+yHjic5tLQm6vqj5KcBWysqrN7K1QaU2lu\n6KyqetWo67IvSXZVlT0ZWlRvfxxJjgSeUFXnA1TVnVX1XeBUmstLaX+e1leZklaMd8Brr/r8T+MB\nwE3tTVJXJPnztv91U1XNAlTVDuDoHsuUxtllwMy+ErU3JN7a3kw4+Prw8lfxLq9cwbI0hnrr5kry\nKJp+7pOq6jNJXgvcCrykqu45kO7mqrpXL4VKklaFPid6/BpwQ1XNDbK+Bzib5r6DTVU129578M1h\nOyfxNFqSDkBVjXwuut66udqurBvaqTUATgY+D3yA3VNCnAG8fy95TOzr3HPPHXkdbJ/tW4vtm+S2\nVa2e/8H7noL+pcDb2nmdvkxzWelBwAVpJvHbzpBpHiRJ463XYFJVVwGPGbLp5/ssR5K0unjd+AqZ\nnp4edRWWle0bb5Pcvklu22rS602LXTRzza2OukjSuEhCTdIAvCRp7TKYSJI6M5hIkjozmEiSOjOY\nSJI6M5hIkjozmEiSOjOYSJI6M5hIkjozmEiSOjOYSJI6M5hIkjozmEiSOjOYSJI6M5hIkjozmEiS\nOjOYaKxNTW0myR6vqanNy76vpD35pEWNtSTA/L+bsJS/pS77SquFT1qUJE0Mg4kkqTODiSSpM4OJ\nJKkzg4kkqTODiSSpM4OJJKkzg4kkqbP1fWaW5Hrgu8Au4IdVdWKSjcC7gGOB64HTq+q7fZYrSRqt\nvs9MdgHTVXVCVZ3YrjsbuKSqjgMuBc7puUxJ0oj1HUwyJM9Tga3t8lbgtJ7LlCSNWN/BpICPJfl0\nkl9v122qqlmAqtoBHN1zmZKkEet1zAR4XFXdmOTHgYuTbGPhTHrOoidJE6bXYFJVN7Y/v5XkQuBE\nYDbJpqqaTTIFfHOx/bds2XLX8vT0NNPT031WT1qCDe1swrtt2nQsO3ZcP5rqSPPMzMwwMzMz6mos\n0NsU9EkOA9ZV1W1JDgcuBl4JnAzcUlXnJTkL2FhVZw/Z3ynotd+WYwp6p6XXOFktU9D3eWayCXhf\nkmrzfVtVXZzkM8AFSV4EbAdO77FMSdIq4MOxNNY8M9Fat1rOTLwDXtqnDQse7+sjfqU9eWaisbZS\nZybDL0L0jEWj55mJJGliGEwkSZ0ZTCRJnRlMJEmdGUwkSZ0ZTCRJnRlMJEmdGUwkSZ0ZTCRJnRlM\ntCZMTW1eMB3KKMt2KhZNGqdT0Vhb6nQq3aZO6TadSpcpX6R9cToVSdLEMJhIkjozmEiSOjOYSJI6\nM5hIkjozmEiSOjOYSJI6M5hIkjozmEiSOjOYSJI6M5hIkjozmEiSOjOYSJI6M5hIkjozmEiSOjOY\nSJI66z2YJFmX5IokH2jfb0xycZJtSS5KclTfZWqyDHsy4eQ9nXDDhLdPa81ynJn8DnDNwPuzgUuq\n6jjgUuCcZShTE2R2djvNkwn3fDXrJ8UdTHb7tNb0GkyS3A94OvB/BlafCmxtl7cCp/VZpiRp9Po+\nM3kt8J/Y84HXm6pqFqCqdgBH91ymJGnE1veVUZJnALNVdWWS6b0krcU2bNmy5a7l6elppqf3lo3G\nzdTU5gVdOZs2HcuOHdePpkLSGJqZmWFmZmbU1VggVYse2/cvo+TVwK8AdwJ3A44A3gc8Gpiuqtkk\nU8BlVXX8kP2rr7podUrCwv8lwvzPfXi6/Um79HQHvm54OcPsT9l+B7S/klBVGXU9euvmqqpXVNUx\nVfVA4PnApVX1q8AHgTPbZGcA7++rTEnS6rAS95m8BnhKkm3Aye17SdIE6a2bqyu7uSbf5HVzHUpz\nie9u69Ydxq5dO4ektZtLy2O1dHP1NgAvrT1z94rstmvXYsFImmxOpyJJ6swzE02gDW23lqSVYjDR\nBFrY/WRXk7S87OaSJHVmMJEkdWYwkSR1ZjCRJHVmMJEkdWYwkSR1ZjCRJHVmMJEkdWYwkSR1ZjCR\nJHVmMNEYaebcGnxNloXtO+igwxesS8LU1OZRV1bag88z0Yrp43km/T6TpPtje0dZtt8Xwep5noln\nJpKkzgwmkqTODCaSpM4MJpKkzgwmkqTODCaSpM4MJpKkzgwmkqTODCaSpM4MJpKkzgwmkqTODCaS\npM56CyZJNiT5ZJLPJbk6ybnt+o1JLk6yLclFSY7qq0xJ0urQWzCpqjuAn6uqE4BHAL+Q5ETgbOCS\nqjoOuBQ4p68yJUmrQ6/dXFW1s13cAKynmTv7VGBru34rcFqfZUqSRq/XYJJkXZLPATuAj1XVp4FN\nVTULUFU7gKP7LFOSNHrr+8ysqnYBJyQ5Enhfkp9g4ZN9Fn2iz5YtW+5anp6eZnp6us/qSdLYm5mZ\nYWZmZtTVWGDZnrSY5PeAncCvA9NVNZtkCrisqo4fkt4nLU44n7TYb9l+XwQT+KTFJD82d6VWkrsB\nTwGuBT4AnNkmOwN4f19lSpJWhz67ue4NbE2yjiZIvauqPpLkcuCCJC8CtgOn91imJGkVWLZurv1l\nN9dkmZrazOzs9iFb7Obqq2y/L4LV081lMNGyWGx8xGDSX9l+XwSrJ5g4nYokqTODiSSpM4OJJKkz\ng4kkqTODiSSpM4OJOpua2kySPV6S1hYvDVZn+3MZsJcG91e23xeBlwZLkiaIwUSS1JnBRJoQw8au\npqY2j7paWiMcM1FnjpmMpuyl/d4cW5l0jplIkiaGwUSS1Fmvj+2V9t8G70uRJoDBRCN2B8PHCSSN\nE7u5JEmdGUwkSZ0ZTCRJnRlMJEmdGUwkSZ0ZTCRJnRlMJEmdGUwkSZ0ZTCRJnRlMpDXGqeq1HJyC\nXp11nYJ+9U/5vtrq06w/0Cnonap+sjgFvSRpYvQWTJLcL8mlST6f5OokL23Xb0xycZJtSS5KclRf\nZUqSVoc+z0zuBF5WVT8BnAT8VpKHAWcDl1TVccClwDk9limtURsWjHt0Syd101swqaodVXVlu3wb\ncC1wP+BUYGubbCtwWl9lSmvX3NT9g68u6aRulmXMJMlm4BHA5cCmqpqFJuAARy9HmZKk0en94VhJ\n7g78JfA7VXVbkvn/Ci36r9GWLVvuWp6enmZ6errv6knSWJuZmWFmZmbU1Vig10uDk6wHPgR8tKpe\n3667FpiuqtkkU8BlVXX8kH29NHhMeWnwZJTt9288TeqlwW8GrpkLJK0PAGe2y2cA7++5TEnSiPV2\nZpLkccDfAleze6TvFcCngAuA+wPbgdOr6jtD9vfMZEx5ZjIZZfv9G0+r5czEO+DVmcFkMsr2+zee\nVksw8Q54SVJnBhNJUmcGE0lSZwYTLcqpyiUtlQPwWlTXKc3HYdB5POuzPOX4/RtPDsBLkiaGwUSS\n1JnBRNJQjplpfzhmokU5ZrJa67M85fh43/HkmIkkaWIYTNaY7l0XPrlP0kJ2c60x+9N1sTLdV+PQ\nrbTa6rM85djNNZ7s5pIkTQyDiSSps94f26txtMGxD0mdGEwE3MHi/fKStG92c0mSOjOYSJI6s5tL\nEo6bqSuDiSSGj5sZXLR0dnNJkjozmEiSOjOYSJI6M5hIkjozmEiSOjOYSFoWPqlxbXEK+jVm6dPK\nL7Z+sqZdH8/6jLLspU9B7xT2K8Mp6CVJE6PXYJLkTUlmk/zjwLqNSS5Osi3JRUmO6rNMSdLo9X1m\ncj5wyrx1ZwOXVNVxwKXAOT2XKWnFLHxscxIOOuhwH+e8xvUaTKrq48C3560+FdjaLm8FTuuzTEkr\naW7alT1fu3btHLJea8lKjJkcXVWzAFW1Azh6BcqUJK2gUQzA+y+LJE2YlZg1eDbJpqqaTTIFfHOx\nhFu2bLlreXp6munp6eWvnSSNkZmZGWZmZkZdjQV6v88kyWbgg1X18Pb9ecAtVXVekrOAjVV19pD9\nvM9kBXifySTUZ5Rld6+P3/N+TeR9JkneDnwCeGiSryZ5IfAa4ClJtgEnt+8lSRPEO+DXGM9MJqE+\noyzbM5PVZiLPTCRJa5PBRJLUmcFEktSZwWSCDZsCXJKWw0rcZ6IRmZ3dzvBBUUnql2cmkqTODCaS\npM4MJpKkzgwmkqTODCaSpM4MJhPCy4A1rob97U5NbR51tbSfnJtrQix9zq21Mf/TZNdnlGX3PzfX\nYn+7Hg+Wxrm5JEkTw2AiSerMYCJpBW1YkbE9x2FWnmMmE8Ixk7VUn1GWvXL16XI8WEvjMI6ZSJIm\nhsFEktSZwWQVsZ9X0rhyzGQV6dLP65jJWqrPKMt2zGS1ccxEkjQxfDjWMrrhhht44hOfzs6dP9hj\n/SMf+Ug++tELRlQrSeqfwWQZbd++nZtvPoRbb33fwNqb+fjHnzmyOknjampqc/v00N3WrTuMXbt2\nLjGHDQvua9m06Vh27Li+nwqucQaTZbZu3d2ABw+sOXJUVZHG2rDHUO/atbcxnPnuWJB2dnbkQw0T\nwzETSVJnBpNVb+H0EwcddLjTzWvCrcy0K0vlZfv7ZjfXqrfw1Hz4qb0BRZNk4d/9KP/Gh3Wx2UW2\nJ89MJEmdrVgwSfK0JF9I8sUkZ61UuZKk5bciwSTJOuCNwCnATwAvSPKwlSh79Zi5a+m2225b0P86\n6j7h7mZGXYFlNjPqCuiAzexl28KxmS5jIcPGVtbK+MpKnZmcCFxXVdur6ofAO4FTV6jsVWJmYPkH\nNP2v81/jbGbUFVhmM6OugA7YzF62zY3N7H7Nv5dlf+weW+kvz3GxUsHkvsANA++/1q6TJE0Ar+Za\nRgcffDC3334NRx75TG6/fRuHHvpZqu7g1ltHXTNJ6teKzBqc5LHAlqp6Wvv+bKCq6ryBNOPezyNJ\nI7EaZg1eqWByELANOBm4EfgU8IKqunbZC5ckLbsV6eaqqh8leQlwMc04zZsMJJI0OVbNw7EkSWOs\nqvb5Ap4GfAH4InDWImneAFwHXAk8Yl/7AhtpzlS2ARcBRw1sO6fN61rgqQPrHwn8Y5vX6wbWH0Jz\nufF1wD8AxwxsOw/4J+Dzg/uMWfueAHwW+CHw7Hn1OqNNvw34tUlpG/DTwCeAq9t6nT5pn127/Qia\nKx3fMGntA+7f5n8NzXfwmAlr3yQcW/5DW/8rgY8B91/qsWVBO/aZoOmW+mfgWODgttCHzUvzC8CH\n2+WfAS7f177tB/G77fJZwGva5X8BfI6mC25zu//cGdQngce0yx8BTmmXXwz8abv8POCd7fJJwN+1\ny6E5OD1xDNt3DPCTwFvY84C7EfgScBRwj7nlCWnbg4EHtcv3Br4BHDkpn91A/V4H/F+GBJNxbx9w\nGfDkdvkw4NBJaR+Tc2x50tznAvw7dh8793psGfZayn0mS7nh8FTgrQBV9UngqCSb9rHvqcDWdnkr\ncFq7/Ky2QXdW1fU0UfbEJFPAEVX16TbdWwf2GczrL4Ent8sFHJrkUOBu7S95dtzaV1Vfrap/YuGd\njacAF1fVd6vqOzT/rTxtEtpWVf9cVV9ql28Evgn8+Ly6j237AJI8Cjia5nMbZmzbl+R44KCqurRN\nt7Oqbp+U9jE5x5a/GfhcLmf3/X/7OrYssJRgspQbDhdLs7d9N1XVbNugHTRfqmF5fX0gr68tktdd\n+1TVj4DvJrlnVV1Oc/vrjW0+F1XVtjFs32IWy2tf9d5bHqulbXdJciJw8FxwWULdl5JmpO1LM3/O\nfwdezuLT4Y5t+4CH0nwP35Pks0nOy8I5g8a2fRN6bPnXwEf3kdeilusO+AO55nnBf25dy0/yIOBh\nwH1ofhEnJ3lcX/nvpz7bt5xWVduS3JvmP6kz+8ryAPZZjvb9Jk33xjfa933dJ7Ba2rceeDzwMuAx\nwIPo5zNcFe2btGNLkl8BHgX88YHmsZRg8nWafsM592vXzU9z/yFp9rbvjvZ0jvY07JtLyGvY+j32\nae9pObKqbgH+FU0f5A+qaidN1D1pDNu3mH3VfZzbRpIjgA8B5wycos+v+7i27yTgJUm+THOG8qtJ\nXj1B7fsacGXbTbMLuJBmEHhS2jcxx5YkP08zcP/MtkttqXXf094GVNqBmIPYPRB0CM1A0PHz0jyd\n3YNIj2X3INKi+9IMIp1Viw8iHQI8gD0HkS6n6UsMzSDS09r1v8nuAfjns3sQ6XSavr6DaAaxLgGe\nMW7tG6jH+cBzBt4PDpLNLd9jQtp2MPDXwEvH+W9zsfbN23YGwwfgx7Z9NP+ofg64V/v+zcCLJ6h9\nE3FsAU5o0z1oXr32emwZ+ne8t40DGT+N5vKw64Cz23W/AfzbgTRvbCt1FfDIve3brr9n+wFsaz+U\nwYPgOW1e8y9vexTNpaLXAa8fWL8BuKBdfzmweeAP+n+x+9LEPx7T9j2apv/yVuBbwNUD285s03+R\nxS8NHru2Ab9MM6XrFTRfkCuAn5qU9s1rw9BgMu7to5nx4qr29WZg/aS0j8k5tnyMZtxn7nt24VKP\nLfNf3rQoSerMx/ZKkjozmEiSOjOYSJI6M5hIkjozmEhak5L8YpJ/SvKjJPPvgZlLc78klyb5fJKr\nk7x03vbfTnJtu+0187Ydk+TWJC/roa7PSnJVks8l+VRPN0j2ysf2Spp4SZ4EnFlVLxxYfTXNzYd/\ntpdd7wReVlVXJrk78NkkF1fVF5JMA88EHl5Vdyb5sXn7/g+aezr6cElVfaBty8NpboU4vqe8e+GZ\niaS1Yo/7IKpqW1Vdx16mMKmqHVV1Zbt8G839G3NzVL2Y5obBO9vtN83tl+RU4Ms007szsP4pST6R\n5DNJ3pXksCVVvLnLfs7dgV1L2W8lGUwkrRWd5j9Lshl4BM107tBMZvnEJJcnuSzJo9t0hwO/C7xy\nsMwk9wL+C3ByVT2a5jkp/3E/yj8tybXAB4EXdWnLcrCbS9LESnI5zfQiRwAbk1zRbjqrqj62H/nc\nnebxFr/TnqFAc/zcWFWPTfIYmq6nBwJbgNdW1c55EyU/lmbKk79vZ1A+mOY5KLTzsj2T3WdPaZcv\nrKrfB6iqC4ELkzwe+EPgKUv+RawAg4mkiVVVj4W7xkzOqKr9/o8+yXqaQPIXVfX+gU03AO9ty/l0\nO5B/L5qHXD0nyR/RzGv1oyS3A1+leUbILw+p5yuAVyyxTR9P8sD2MRu37G97lovdXJK09y6wNwPX\nVNXr562/kPZBfEkeChxSVTdX1ROr6oFV9UCaJ2m+uqr+lGbewMe109eT5LAkD1lS5dp92uVHtmWt\nmkACBhNJa1Q7BnEDTffTh5J8tF1/7yQfapcfRzPp6JPby3KvSDL3xMHzgQcmuRp4O/BreyuvHaA/\nE3hHkqtouriOW2J1n9NexnwF8Cc0sxavKk70KEnqzDMTSVJnBhNJUmcGE0lSZwYTSVJnBhNJUmcG\nE0lSZwYTSVJnBhNJUmf/H9h0in3CPPuNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cdb5690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(hb_s,50);\n",
    "plt.title(\"distribution of |h_bias|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test sample hiddens\n",
    "\n",
    "IDK why there is a divide-by-zero error\n",
    "\n",
    "Perhaps the exitp() needs some regularization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.297486",
     "start_time": "2016-10-06T15:27:41.272250"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:143: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    }
   ],
   "source": [
    "def test_sample_hiddens():\n",
    "    rng = np.random.RandomState(0)\n",
    "    X = Xdigits[:100]\n",
    "    rbm1 = EMF_RBM(n_components=2, batch_size=5,\n",
    "                        n_iter=5, random_state=42)\n",
    "    rbm1.fit(X)\n",
    "\n",
    "    h = rbm1._mean_hiddens(X[0])\n",
    "    hs = np.mean([rbm1._sample_hiddens(X[0]) for i in range(100)], 0)\n",
    "\n",
    "    assert_almost_equal(h, hs, decimal=1)\n",
    "\n",
    "test_sample_hiddens()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>why divide by zero error ? <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.513190",
     "start_time": "2016-10-06T15:27:41.299490"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six.moves import cStringIO as StringIO\n",
    "def test_rbm_verbose():\n",
    "    rbm = EMF_RBM(n_iter=2, verbose=10)\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StringIO()\n",
    "    try:\n",
    "        rbm.fit(Xdigits)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "test_rbm_verbose()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T01:29:02.736552",
     "start_time": "2016-10-04T01:29:02.734464"
    }
   },
   "source": [
    "### Test Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.548651",
     "start_time": "2016-10-06T15:27:41.514911"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_transform():\n",
    "    X = Xdigits[:110] # using 100 causes divide by zero error in mean_hiddens()!\n",
    "    rbm1 = EMF_RBM(n_components=16, batch_size=5,\n",
    "                        n_iter=5, random_state=42)\n",
    "    rbm1.fit(X)\n",
    "\n",
    "    Xt1 = rbm1.transform(X)\n",
    "    Xt2 = rbm1._mean_hiddens(X)\n",
    "\n",
    "    assert_array_equal(Xt1, Xt2)\n",
    "    \n",
    "test_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test means_hidden\n",
    "\n",
    "should compare to older RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.562481",
     "start_time": "2016-10-06T15:27:41.550071"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02649814]\n",
      " [ 0.02009084]]\n",
      "[-0.00342014 -0.00348868]\n",
      "[ 0.05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.array([[0.], [1.]])\n",
    "rbm = BernoulliRBM(n_components=2, batch_size=2,\n",
    "                    n_iter=42, random_state=rng)\n",
    "# you need that much iters\n",
    "rbm.fit(X)\n",
    "#assert_almost_equal(rbm1.W, np.array([[0.02649814], [0.02009084]]), decimal=4)\n",
    "#assert_almost_equal(rbm1.gibbs(X), X)\n",
    "print rbm.components_\n",
    "print rbm.intercept_hidden_\n",
    "print rbm.intercept_visible_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:42:12.136222",
     "start_time": "2016-10-05T23:42:12.133731"
    }
   },
   "source": [
    "### <font color='red'>why is this off ? </font>\n",
    "\n",
    "Could this be due to the regularization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.586972",
     "start_time": "2016-10-06T15:27:41.564416"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.5       ]\n",
      " [ 0.50012314  0.49996445]]\n",
      "[[ 0.00049258]\n",
      " [-0.0001422 ]]\n",
      "[ -7.96246297e-06]\n"
     ]
    }
   ],
   "source": [
    "def test_mean_hiddens():\n",
    "    # Im not entirely sure why this happens, but the hidden units all go to 1/2\n",
    "    # and the h array is (2,2)\n",
    "    # h never changes ... WTf ?!\n",
    "    rng = np.random.RandomState(42)\n",
    "    X = np.array([[0.], [1.]])\n",
    "    rbm = EMF_RBM(n_components=2, batch_size=2,\n",
    "                        n_iter=42, random_state=rng, \n",
    "                        decay = 0.0, weight_decay=None, momentum=0)\n",
    "    rbm.fit(X)\n",
    "    h = rbm._mean_hiddens(X)\n",
    "    assert_true(h.shape==(2,2))\n",
    "    assert_almost_equal(np.linalg.norm(h,ord=2), 1.0, decimal=4)\n",
    "    assert_almost_equal(h[0,0], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[0,1], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[1,0], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[1,1], 0.5, decimal=3)\n",
    "\n",
    "    print h\n",
    "    print rbm.W\n",
    "    print rbm.v_bias\n",
    "    \n",
    "test_mean_hiddens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test equlibrate\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:01:14.557937",
     "start_time": "2016-10-06T17:01:14.543366"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.15573045e-06]\n",
      " [ -1.97855285e-05]]\n"
     ]
    }
   ],
   "source": [
    "def test_fit_equilibrate():\n",
    "    # Equlibrate on the RBM hidden layer should be able to recreate [[0], [1]]\n",
    "    # from the same input\n",
    "    rng = np.random.RandomState(42)\n",
    "    X = np.array([[0.], [1.]])\n",
    "    rbm1 = EMF_RBM(n_components=2, batch_size=2,\n",
    "                        n_iter=42, random_state=rng)\n",
    "    # you need that much iters\n",
    "    rbm1.fit(X)\n",
    "    #assert_almost_equal(rbm1.W, np.array([[0.02649814], [0.02009084]]), decimal=4)\n",
    "    #assert_almost_equal(rbm1.gibbs(X), X)\n",
    "    return rbm1, X\n",
    "\n",
    "rbm, X = test_fit_equilibrate()\n",
    "print rbm.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using sparse CSR matrix\n",
    "\n",
    "### <font color='red'>must implement sparse matrices: not so simple <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:01:15.585208",
     "start_time": "2016-10-06T17:01:15.527204"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:143: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "adding a nonzero scalar to a sparse matrix is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-8b4873d22414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mEMF_RBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# no exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_small_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-134-8b4873d22414>\u001b[0m in \u001b[0;36mtest_small_sparse\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# EMF_RBM should work on small sparse matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdigits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mEMF_RBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# no exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_small_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-02429870c953>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m#print \"iter \", iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_slices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;31m#print \"batches done\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-02429870c953>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, v_pos)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# get_negative_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mv_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequilibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneq_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# basic gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-02429870c953>\u001b[0m in \u001b[0;36mequilibrate\u001b[0;34m(self, v0, h0, iters)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mmh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmh_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-02429870c953>\u001b[0m in \u001b[0;36mmv_update\u001b[0;34m(self, v, h)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mh_fluc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mh_fluc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;31m#a += safe_sparse_dot(h_fluc,self.W2)*(0.5-v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Now we would add this scalar to every element.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 raise NotImplementedError('adding a nonzero scalar to a '\n\u001b[0m\u001b[1;32m    379\u001b[0m                                           'sparse matrix is not supported')\n\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: adding a nonzero scalar to a sparse matrix is not supported"
     ]
    }
   ],
   "source": [
    "def test_small_sparse():\n",
    "    # EMF_RBM should work on small sparse matrices.\n",
    "    X = csr_matrix(Xdigits[:4])\n",
    "    EMF_RBM().fit(X)       # no exception\n",
    "\n",
    "test_small_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more tests TODO\n",
    "\n",
    "#### in sklearn\n",
    "- test_small_sparse():\n",
    "- test_small_sparse_partial_fit():\n",
    "- test_sample_hiddens():\n",
    "- test_equilibrate():\n",
    "- test_equilibrate_sparse():\n",
    "- test_equilibrate_smoke():\n",
    "- test_score_samples():\n",
    "- test_sparse_and_verbose():\n",
    "\n",
    "\n",
    "\n",
    "#### system tests\n",
    "\n",
    "- check that code works with 2 or more iterations\n",
    "- check  mnist, with accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T01:06:25.316119",
     "start_time": "2016-10-04T01:06:25.298496"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try classifier\n",
    "\n",
    "#### should we be using the EMF estimator?\n",
    "\n",
    "what are the correlations...do they drop to 0 as we converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.498831",
     "start_time": "2016-10-05T21:22:24.636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.499224",
     "start_time": "2016-10-05T21:22:24.819Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = sig_means(X, rbm.h_bias , rbm.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.499573",
     "start_time": "2016-10-05T21:22:24.986Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print p.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.500034",
     "start_time": "2016-10-05T21:22:25.162Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(p, Y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.500518",
     "start_time": "2016-10-05T21:22:25.332Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for c in [5000]:\n",
    "    lr  = linear_model.LogisticRegression()\n",
    "    lr.C = c\n",
    "    lr.fit(X_train, Y_train)\n",
    "    Y_test_pred = lr.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "    print c, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.501155",
     "start_time": "2016-10-05T21:22:25.515Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### note bad, but not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
